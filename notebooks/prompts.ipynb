{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d7aae8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29T08:17:13.746271Z [error    ] retry_backoff                  backoff=1 caller=\"src/llm/client.py:38\" error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download') function=retry_backoff\n",
      "2025-07-29T08:17:15.747500Z [error    ] retry_backoff                  backoff=2 caller=\"src/llm/client.py:38\" error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download') function=retry_backoff\n",
      "2025-07-29T08:17:19.748575Z [error    ] retry_backoff                  backoff=4 caller=\"src/llm/client.py:38\" error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download') function=retry_backoff\n",
      "2025-07-29T08:17:27.749841Z [error    ] retry_backoff                  backoff=8 caller=\"src/llm/client.py:38\" error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download') function=retry_backoff\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:17:43.785442Z [info     ] retrieved_company_by_ticker    caller=\"src/database/companies.py:285\" company=ADVANCED MICRO DEVICES INC function=get_company_by_ticker ticker=AMD\n",
      "2025-07-29T08:17:43.792751Z [info     ] retrieved_filings_by_company   caller=\"src/database/filings.py:275\" count=5 function=get_filings_by_company\n",
      "2025-07-29T08:17:43.793262Z [info     ] filtered_list_of_filings       caller=\"/tmp/ipykernel_33445/378144849.py:36\" filings=[AMD 2020 10-K, AMD 2021 10-K, AMD 2022 10-K, AMD 2023 10-K, AMD 2024 10-K] function=<module>\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:18:24.837897Z [info     ] role='assistant' content='Okay, let\\'s explore `go while-d` loops in Go.  \"While-d\" isn\\'t standard Go terminology, but it sounds like you\\'re likely referring to a `for` loop with a conditional expression that functions like a `while` loop in other languages.\\n\\nHere\\'s a comprehensive breakdown of how `while`-like loops work in Go, including examples and explanations:\\n\\n**Go\\'s `for` Loop (The General Structure)**\\n\\nGo doesn\\'t have a `while` keyword like some other programming languages (e.g., C++, Java, Python). Instead, it uses the `for` loop, which is remarkably versatile.  The `for` loop can be used to create what\\'s functionally equivalent to `while` loops.\\n\\nThe general structure of a `for` loop in Go is:\\n\\n```go\\nfor initialization; condition; post {\\n  // Code to be executed as long as the condition is true\\n}\\n```\\n\\n* **Initialization:** This is executed *once* at the beginning of the loop.  You can declare and initialize variables here (or just skip it if you don\\'t need to).\\n* **Condition:** This is an expression that is evaluated *before each iteration* of the loop. If the condition is `true`, the loop body is executed. If the condition is `false`, the loop terminates.\\n* **Post:** This is executed *after each iteration* of the loop (including the first if the condition is initially true).  You commonly use this to increment/decrement counters, update variables, or perform other actions that need to happen at the end of each loop cycle.\\n\\n**Creating a `while`-like Loop**\\n\\nTo create a loop that behaves like a `while` loop, you simply omit the initialization and post sections of the `for` loop and only keep the condition.\\n\\n```go\\nfor condition {\\n  // Code to be executed as long as the condition is true\\n}\\n```\\n\\n**Example 1: Simple `while`-like Loop**\\n\\nLet\\'s say you want to execute code as long as a variable `x` is less than 10.\\n\\n```go\\npackage main\\n\\nimport \"fmt\"\\n\\nfunc main() {\\n  x := 0\\n  for x < 10 {\\n    fmt.Println(\"x is:\", x)\\n    x++ // Increment x (essential to prevent an infinite loop)\\n  }\\n  fmt.Println(\"Loop finished.\")\\n}\\n```\\n\\nIn this example:\\n\\n* `x := 0` initializes the variable `x` to 0.\\n* `for x < 10` is the condition that controls the loop. The loop continues as long as `x` is less than 10.\\n* `fmt.Println(\"x is:\", x)` prints the current value of `x`.\\n* `x++` increments `x` by 1.  This is crucial to eventually make the condition `x < 10` false, and terminate the loop.  Without it, you\\'d have an infinite loop.\\n\\n**Example 2: Loop with a Boolean Variable**\\n\\nSometimes you have a boolean variable that determines when the loop should continue.\\n\\n```go\\npackage main\\n\\nimport \"fmt\"\\n\\nfunc main() {\\n  keepLooping := true\\n  count := 0\\n\\n  for keepLooping {\\n    fmt.Println(\"Looping... Count:\", count)\\n    count++\\n\\n    if count > 5 {\\n      keepLooping = false // Change the boolean to stop the loop\\n    }\\n  }\\n\\n  fmt.Println(\"Loop ended.\")\\n}\\n```\\n\\nHere, `keepLooping` is a boolean variable.  The loop continues as long as `keepLooping` is `true`.  Inside the loop, when `count` becomes greater than 5, we change `keepLooping` to `false`, causing the loop to terminate.\\n\\n**Example 3: Looping Until a Condition is Met**\\n\\n```go\\npackage main\\n\\nimport \"fmt\"\\n\\nfunc main() {\\n    sum := 0\\n    i := 1\\n\\n    for sum < 100 {\\n        sum += i\\n        fmt.Println(\"Sum:\", sum)\\n        i++\\n    }\\n\\n    fmt.Println(\"Final Sum:\", sum)\\n}\\n```\\n\\nIn this example, the loop continues as long as `sum` is less than 100.  Each iteration, the value of `i` is added to `sum`, and `i` is incremented.\\n\\n**Important Considerations**\\n\\n* **Infinite Loops:** The most common error when using `for`-like `while` loops is creating an infinite loop. This happens when the condition *always* evaluates to `true`.  Make absolutely sure that something inside the loop changes a variable that is part of the condition, so that the condition eventually becomes `false`.\\n* **`break` and `continue` Statements:**  You can use the `break` statement to exit a loop prematurely (regardless of the condition).  You can use `continue` to skip the rest of the current iteration and go directly to the next iteration.\\n\\n**Why Go Doesn\\'t Have a `while` Keyword**\\n\\nGo\\'s design philosophy favors simplicity and a small set of language features.  The `for` loop is powerful enough to handle all the use cases that a `while` loop would cover.  Eliminating the `while` keyword reduces the number of concepts a Go programmer needs to learn.\\n\\n**Summary**\\n\\nWhile Go doesn\\'t have a `while` keyword, you can easily create `while`-like loops using the `for` loop with just a condition.  Remember to ensure your condition eventually becomes `false` to avoid infinite loops!  The versatility of Go\\'s `for` loop allows it to handle a wide range of iteration scenarios.\\n' images=None tool_calls=None caller=\"/tmp/ipykernel_33445/378144849.py:41\" function=<module>\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:18:30.969360Z [info     ] Okay, here's a one-liner joke aiming for that dry, witty, Unixporn audience (young professionals, engineers, appreciates meticulousness and subtle humor):\n",
      "\n",
      "**\"My prompt is currently displaying the precise temperature of my hard drive. It's a *very* accurate way to feel inadequate.\"**\n",
      "\n",
      "---\n",
      "\n",
      "**Why this works for that audience:**\n",
      "\n",
      "*   **Unixporn aesthetic:** It's about a prompt, implying customization and technical setup.\n",
      "*   **Dry Humor:** The humor is understated and relies on the contrast between meticulous data display and a relatable feeling of inadequacy.\n",
      "*   **Relatable to Engineers/Professionals:**  Engineers appreciate precision and data, but also tend to be self-deprecating.  It acknowledges that even in a perfectly optimized environment, you can still feel lacking.\n",
      "*   **Subtle:**  It's not a loud, obvious joke. It requires a *little* bit of context to fully appreciate.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like another one! caller=\"/tmp/ipykernel_33445/378144849.py:52\" function=<module>\n"
     ]
    }
   ],
   "source": [
    "# Imports and Config\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "from src.database.base import init_db\n",
    "from src.database.filings import get_filings_by_company\n",
    "from src.database.companies import get_company_by_ticker, update_company\n",
    "from src.database.completions import Completion\n",
    "from src.database.documents import get_documents_by_filing, DocumentType\n",
    "from src.database.prompts import *\n",
    "\n",
    "from src.llm.client import ModelConfig, init_client, remove_thinking_tags, get_generate_response\n",
    "from src.llm.aggregates import create_document_type_aggregate\n",
    "from src.llm.completions import process_document_completion\n",
    "\n",
    "from src.utils.config import settings\n",
    "from src.utils.logging import configure_logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "init_db(settings.database.url)\n",
    "session = get_db_session()\n",
    "\n",
    "client = init_client(settings.openai_api.url)\n",
    "\n",
    "COMPANY = \"AMD\"\n",
    "TICKERS = [\"AMD\", \"AAPL\", \"GOOGL\", ]\n",
    "company = get_company_by_ticker(COMPANY)\n",
    "filings = get_filings_by_company(company.id)\n",
    "# filings = [filing for filing in filings if filing.period_of_report.year > 2023]\n",
    "logger.info(\"filtered_list_of_filings\", filings=filings)\n",
    "\n",
    "from src.llm.client import retry_backoff\n",
    "\n",
    "response = retry_backoff(3600, client.chat, 'gemma3:12b', options={\"temperature\": 1.0}, messages=[{\"role\": \"user\", \"content\": \"go while-d\"}])\n",
    "logger.info(response.message)\n",
    "\n",
    "response = retry_backoff(\n",
    "    timeout=3,\n",
    "    func=client.generate,\n",
    "    model = \"gemma3:12b\",\n",
    "    system = \"you're an intelligent comedian who finds humor in witty dry comedy. Your target audience is young professionals and engineers.\",\n",
    "    prompt = \"Write a one line joke for the front page of /r/unixporn\",\n",
    "    options = {\n",
    "    },\n",
    ")\n",
    "logger.info(response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a26852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new prompts\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"focused_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#         We're archivists! We need to transform this input into output that is concise, factual, sometimes said with a hint of personality.  Our purpose is to accurately represent the information contained in the input text.  We prioritize completeness; ensure every relevant point is captured, even if seemingly minor. We look for literal and implied meanings.\n",
    "#         \"\"\",\n",
    "# }\n",
    "# document_prompt = create_prompt(data)\n",
    "\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"aggregate_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#     You are a content writer with a worldwide audience of talented young professionals.\n",
    "#     You will recieve a large series of research notes, sometimes spanning months or years.\n",
    "#     Take these notes and write a summary of the topics discuessed therein.\n",
    "\n",
    "#     Use markdown to format your response:\n",
    "#     - list items\n",
    "#     - **bold text**\n",
    "#     - _italics_\n",
    "#     - Headings require one or more '#' characters\n",
    "# \"\"\",\n",
    "# }\n",
    "# aggregate_prompt = create_prompt(data)\n",
    "\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"aggregate_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#     You're a change analyst! You will be presented with a series of documents. Each document has a meta section with important context (including a date).\n",
    "\n",
    "#     Compare each of the documents.\n",
    "\n",
    "#     Summarize the progression of each document.\n",
    "\n",
    "#     Use markdown to format your response:\n",
    "#     - list items\n",
    "#     - **bold text**\n",
    "#     - _italics_\n",
    "#     - Headings require one or more '#' characters\n",
    "# \"\"\",\n",
    "# }\n",
    "# risk_aggregate_prompt = create_prompt(data)\n",
    "\n",
    "\n",
    "# create markdown prompt\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"markdown_sanitation\",\n",
    "#     \"content\": \"\"\"\n",
    "# You are a formatting wizard who's job it is to enforce proper markdown page structure!\n",
    "\n",
    "# You MUST NOT change any words or phrases in the provided document.\n",
    "\n",
    "# You also MUST NOT add any form of wrapping or text around the document (backticks, quotes, etc)\n",
    "\n",
    "# One common formatting error is the use of **bolded text** in place of a subheading (#'s).\n",
    "# Here is an example of this error:\n",
    "# <error>\n",
    "# **Palantir: A Story of Innovation, Ethics, and Global Impact**\n",
    "\n",
    "# In 2003, a group of technologists, entrepreneurs, and former intelligence officers founded Palantir with a bold mission: to build software that could transform how the world handles data, decisions, and operations. What began as a tool for counterterrorism in the U.S. intelligence community evolved into a global leader in data integration, AI, and ethical technology. Today, Palantir’s platforms empower governments, corporations, and institutions to solve complex problems—from combating crime to advancing public health—while upholding rigorous privacy and ethical standards.\n",
    "\n",
    "# **The Power of Platforms**\n",
    "# At the heart of Palantir’s success are its four core platforms, each designed to tackle unique challenges:\n",
    "# - **Gotham** serves as a central nervous system for intelligence and defense agencies, synthesizing vast datasets (e.g., signals intelligence, financial records) to uncover patterns and guide high-stakes decisions.\n",
    "# - **Foundry** acts as a universal operating system for data, enabling organizations to build, manage, and optimize data pipelines across departments, from data engineers to executives.\n",
    "# - **Apollo** ensures seamless, secure deployment of software across any cloud or on-premise environment, minimizing downtime and maximizing security.\n",
    "# - **AIP (Artificial Intelligence Platform)**, a recent innovation, integrates cutting-edge machine learning and generative AI (e.g., large language models) with Palantir’s existing systems, allowing enterprises to operationalize AI in real-time, from fraud detection to predictive analytics.\n",
    "\n",
    "# **Customers, Revenue, and Global Reach**\n",
    "# Palantir’s customer base spans 711 organizations as of 2024, with 55% of revenue coming from government clients (U.S. federal agencies, international defense, and public health) and 45% from commercial sectors like healthcare, energy, and finance. Their U.S. operations drive 66% of revenue, with the remaining 34% generated internationally. Top customers, such as the U.S. military and major financial institutions, see average annual revenue from Palantir exceeding $64 million—a testament to the scalability of their platforms.\n",
    "\n",
    "# **A Sales Strategy Built on Trust**\n",
    "# Palantir targets large-scale, mission-critical projects, leveraging a direct sales force, strategic partnerships (e.g., with Fujitsu), and cloud providers to expand access. Their \"Developer Tier\" initiative democratizes access to Foundry and AIP, allowing developers in the U.S. and select countries to experiment with their tools. This approach, combined with bootcamps and training, has helped Palantir navigate the risk-averse culture of government clients, where rapid deployment and proven outcomes are paramount.\n",
    "\n",
    "# **Privacy: The Bedrock of Innovation**\n",
    "# From the start, Palantir has prioritized privacy as a non-negotiable feature. Their \"privacy by design\" philosophy embeds data minimization, dynamic access controls, and auditability into every platform. For example, Foundry ensures compliance with GDPR and HIPAA through automated data retention policies and granular permissions. This commitment extends to AIP, where human oversight—rather than algorithmic automation—guides critical decisions. Palantir’s ethical framework, which includes accountability and transparency, has become a differentiator in an era of growing AI scrutiny.\n",
    "\n",
    "# **Innovation at the Edge**\n",
    "# Research and development drive Palantir’s evolution. Teams work on edge computing, AI integration, and user-centric design, often collaborating directly with customers to refine solutions. AIP bootcamps, for instance, enable real-world testing of AI applications, from healthcare diagnostics to supply chain optimization. These efforts ensure that Palantir’s tools remain at the forefront of technological progress, even as competitors—both traditional software giants and AI startups—strive to replicate their success.\n",
    "\n",
    "# **Global Impact, Local Roots**\n",
    "# Palantir’s platforms have transformed industries:\n",
    "# - **Law Enforcement**: Streamlined investigations and resource allocation.\n",
    "# - **Healthcare**: Enhanced disease tracking and drug discovery.\n",
    "# - **Finance**: Strengthened fraud detection and compliance.\n",
    "# With 3,936 employees globally (31% outside the U.S.), Palantir balances global ambition with local engagement, fostering inclusive workplaces and adhering to labor standards, including France’s works council requirements.\n",
    "\n",
    "# **Challenges and the Road Ahead**\n",
    "# Despite its strengths, Palantir faces hurdles: competition from internal development teams, the need to balance AI innovation with ethical boundaries, and the seasonal nature of enterprise software sales. Yet, its focus on privacy, scalability, and mission-driven partnerships positions it to navigate these challenges. As AIP and Foundry continue to evolve, Palantir remains committed to its founding vision: using technology not just to solve problems, but to do so responsibly,\n",
    "# </error>\n",
    "\n",
    "# And the corrected formatting:\n",
    "\n",
    "# <correct>\n",
    "# # Palantir: A Story of Innovation, Ethics, and Global Impact**\n",
    "\n",
    "# In 2003, a group of technologists, entrepreneurs, and former intelligence officers founded Palantir with a bold mission: to build software that could transform how the world handles data, decisions, and operations. What began as a tool for counterterrorism in the U.S. intelligence community evolved into a global leader in data integration, AI, and ethical technology. Today, Palantir’s platforms empower governments, corporations, and institutions to solve complex problems—from combating crime to advancing public health—while upholding rigorous privacy and ethical standards.\n",
    "\n",
    "# ## The Power of Platforms\n",
    "# At the heart of Palantir’s success are its four core platforms, each designed to tackle unique challenges:\n",
    "# - **Gotham** serves as a central nervous system for intelligence and defense agencies, synthesizing vast datasets (e.g., signals intelligence, financial records) to uncover patterns and guide high-stakes decisions.\n",
    "# - **Foundry** acts as a universal operating system for data, enabling organizations to build, manage, and optimize data pipelines across departments, from data engineers to executives.\n",
    "# - **Apollo** ensures seamless, secure deployment of software across any cloud or on-premise environment, minimizing downtime and maximizing security.\n",
    "# - **AIP (Artificial Intelligence Platform)**, a recent innovation, integrates cutting-edge machine learning and generative AI (e.g., large language models) with Palantir’s existing systems, allowing enterprises to operationalize AI in real-time, from fraud detection to predictive analytics.\n",
    "\n",
    "# ### Customers, Revenue, and Global Reach\n",
    "# Palantir’s customer base spans 711 organizations as of 2024, with 55% of revenue coming from government clients (U.S. federal agencies, international defense, and public health) and 45% from commercial sectors like healthcare, energy, and finance. Their U.S. operations drive 66% of revenue, with the remaining 34% generated internationally. Top customers, such as the U.S. military and major financial institutions, see average annual revenue from Palantir exceeding $64 million—a testament to the scalability of their platforms.\n",
    "\n",
    "# ### A Sales Strategy Built on Trust\n",
    "# Palantir targets large-scale, mission-critical projects, leveraging a direct sales force, strategic partnerships (e.g., with Fujitsu), and cloud providers to expand access. Their \"Developer Tier\" initiative democratizes access to Foundry and AIP, allowing developers in the U.S. and select countries to experiment with their tools. This approach, combined with bootcamps and training, has helped Palantir navigate the risk-averse culture of government clients, where rapid deployment and proven outcomes are paramount.\n",
    "\n",
    "# ### Privacy: The Bedrock of Innovation\n",
    "# From the start, Palantir has prioritized privacy as a non-negotiable feature. Their \"privacy by design\" philosophy embeds data minimization, dynamic access controls, and auditability into every platform. For example, Foundry ensures compliance with GDPR and HIPAA through automated data retention policies and granular permissions. This commitment extends to AIP, where human oversight—rather than algorithmic automation—guides critical decisions. Palantir’s ethical framework, which includes accountability and transparency, has become a differentiator in an era of growing AI scrutiny.\n",
    "\n",
    "# ### Innovation at the Edge\n",
    "# Research and development drive Palantir’s evolution. Teams work on edge computing, AI integration, and user-centric design, often collaborating directly with customers to refine solutions. AIP bootcamps, for instance, enable real-world testing of AI applications, from healthcare diagnostics to supply chain optimization. These efforts ensure that Palantir’s tools remain at the forefront of technological progress, even as competitors—both traditional software giants and AI startups—strive to replicate their success.\n",
    "\n",
    "# ## Global Impact, Local Roots\n",
    "# Palantir’s platforms have transformed industries:\n",
    "# - **Law Enforcement**: Streamlined investigations and resource allocation.\n",
    "# - **Healthcare**: Enhanced disease tracking and drug discovery.\n",
    "# - **Finance**: Strengthened fraud detection and compliance.\n",
    "# With 3,936 employees globally (31% outside the U.S.), Palantir balances global ambition with local engagement, fostering inclusive workplaces and adhering to labor standards, including France’s works council requirements.\n",
    "\n",
    "# ## Challenges and the Road Ahead**\n",
    "# Despite its strengths, Palantir faces hurdles: competition from internal development teams, the need to balance AI innovation with ethical boundaries, and the seasonal nature of enterprise software sales. Yet, its focus on privacy, scalability, and mission-driven partnerships positions it to navigate these challenges. As AIP and Foundry continue to evolve, Palantir remains committed to its founding vision: using technology not just to solve problems, but to do so responsibly,\n",
    "# </correct>\n",
    "#         \"\"\",\n",
    "# }\n",
    "# markdown_prompt = create_prompt(data)\n",
    "\n",
    "# create summary prompt\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"front_page_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#         You're a fantastic ad copy editor. The user will provide you with a document, which you need to boil down into it's most concise esscense.\n",
    "\n",
    "#         Aim for no more than 50 words, 75 at the very most.\n",
    "\n",
    "#         Use markdown (**bold text**, _italics_, - bullet points) to add emphasis to your response.\n",
    "#     \"\"\",\n",
    "# }\n",
    "# front_page_prompt = create_prompt(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381e976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29T08:18:30.989971Z [info     ] retrieved_prompt               caller=\"src/database/prompts.py:83\" function=get_prompt prompt_id=06878708-7a86-76e0-8000-8b1f4e2ed08c\n",
      "2025-07-29T08:18:30.991009Z [info     ] retrieved_prompt               caller=\"src/database/prompts.py:83\" function=get_prompt prompt_id=0687ab13-8e9b-7d45-8000-e36b40bd5fcd\n",
      "2025-07-29T08:18:30.991693Z [info     ] retrieved_prompt               caller=\"src/database/prompts.py:83\" function=get_prompt prompt_id=0687abe9-4f99-7ca7-8000-77b422a5c1dc\n",
      "2025-07-29T08:18:30.992891Z [info     ] retrieved_prompt               caller=\"src/database/prompts.py:83\" function=get_prompt prompt_id=0687a885-7cc1-7e88-8000-a95d86e69d22\n",
      "2025-07-29T08:18:30.993533Z [info     ] retrieved_prompt               caller=\"src/database/prompts.py:83\" function=get_prompt prompt_id=0687a424-1757-743d-8000-01257e133b73\n"
     ]
    }
   ],
   "source": [
    "# prompts and model config\n",
    "document_prompt = get_prompt('06878708-7a86-76e0-8000-8b1f4e2ed08c')\n",
    "document_model_config = ModelConfig(\n",
    "    name=\"qwen3:4b\",\n",
    "    num_ctx=24567,\n",
    "    temperature=0.8,\n",
    "    top_k=45\n",
    ")\n",
    "\n",
    "# aggregate_prompt = get_prompt(\"068786e2-1e9e-7eb6-8000-e716f8eb8a3f\")\n",
    "# aggregate_prompt = get_prompt(\"0687a819-b5cb-73a3-8000-4326f7819960\")\n",
    "aggregate_prompt = get_prompt(\"0687ab13-8e9b-7d45-8000-e36b40bd5fcd\")\n",
    "\n",
    "risk_aggregate_prompt = get_prompt(\"0687abe9-4f99-7ca7-8000-77b422a5c1dc\")\n",
    "\n",
    "aggregate_model_config = ModelConfig(\n",
    "    name=\"qwen3:14b\",\n",
    "    num_ctx=9000,\n",
    "    temperature=0.7,\n",
    "    num_gpu=41\n",
    ")\n",
    "\n",
    "# aggregate_model_config = ModelConfig(\n",
    "#     name=\"hf.co/unsloth/gemma-3-12b-it-qat-GGUF:latest\",\n",
    "#     num_ctx=10000,\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "markdown_prompt = get_prompt('0687a885-7cc1-7e88-8000-a95d86e69d22')\n",
    "markdown_model_config = ModelConfig(\n",
    "    name=\"hf.co/unsloth/gemma-3-12b-it-qat-GGUF:latest\",\n",
    "    num_ctx=10000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "front_page_prompt = get_prompt('0687a424-1757-743d-8000-01257e133b73')\n",
    "front_page_model_config = ModelConfig(\n",
    "    name=\"hf.co/unsloth/gemma-3-12b-it-qat-GGUF:latest\",\n",
    "    num_ctx=10000,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc26195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29T08:18:30.998656Z [info     ] get_completion_for_filing_documents caller=\"/tmp/ipykernel_33445/4124532146.py:3\" company=ADVANCED MICRO DEVICES INC function=<module> index=0/3 ticker=AMD\n",
      "2025-07-29T08:18:31.001387Z [info     ] retrieved_company_by_ticker    caller=\"src/database/companies.py:285\" company=ADVANCED MICRO DEVICES INC function=get_company_by_ticker ticker=AMD\n",
      "2025-07-29T08:18:31.002161Z [info     ] retrieved_filings_by_company   caller=\"src/database/filings.py:275\" count=5 function=get_filings_by_company\n",
      "2025-07-29T08:18:31.003481Z [info     ] filing_loop                    caller=\"/tmp/ipykernel_33445/4124532146.py:15\" filing=AMD 2020 10-K function=<module> index=0/5\n",
      "2025-07-29T08:18:31.010153Z [info     ] retrieved_documents_by_filing  caller=\"src/database/documents.py:255\" count=3 filing_id=0685e25b-10ea-779d-8000-cb13053c2e9e function=get_documents_by_filing\n",
      "2025-07-29T08:18:31.010562Z [info     ] document_loop                  caller=\"/tmp/ipykernel_33445/4124532146.py:22\" document=AMD 2020 10-K business_description function=<module> index=0/5\n",
      "2025-07-29T08:18:31.011841Z [info     ] document_completion_start      caller=\"src/llm/completions.py:42\" company=AMD document_type=business_description function=process_document_completion year=2020\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:18:32.418619Z [info     ] sending_chat_request           caller=\"src/llm/client.py:68\" function=get_chat_response input_tokens=12224 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:19:48.739870Z [info     ] recieved_chat_response         caller=\"src/llm/client.py:99\" done=True done_reason=stop duration=0.00s function=get_chat_response model=qwen3:4b output_tokens=1715 tokens_per_second=22.58 tokens/s\n",
      "2025-07-29T08:19:48.760084Z [info     ] create_completion              caller=\"src/database/completions.py:161\" completion_id=06888842-4bfb-7208-8000-39576c6f60a2 function=create_completion\n",
      "2025-07-29T08:19:48.761373Z [info     ] document_loop                  caller=\"/tmp/ipykernel_33445/4124532146.py:22\" document=AMD 2020 10-K risk_factors function=<module> index=1/5\n",
      "2025-07-29T08:19:48.766361Z [info     ] document_completion_start      caller=\"src/llm/completions.py:42\" company=AMD document_type=risk_factors function=process_document_completion year=2020\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:19:49.488880Z [info     ] sending_chat_request           caller=\"src/llm/client.py:68\" function=get_chat_response input_tokens=19562 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:21:37.136204Z [info     ] recieved_chat_response         caller=\"src/llm/client.py:99\" done=True done_reason=stop duration=0.00s function=get_chat_response model=qwen3:4b output_tokens=1629 tokens_per_second=15.18 tokens/s\n",
      "2025-07-29T08:21:37.141232Z [info     ] create_completion              caller=\"src/database/completions.py:161\" completion_id=06888849-1237-7f97-8000-61c5e804e425 function=create_completion\n",
      "2025-07-29T08:21:37.142408Z [info     ] document_loop                  caller=\"/tmp/ipykernel_33445/4124532146.py:22\" document=AMD 2020 10-K management_discussion function=<module> index=2/5\n",
      "2025-07-29T08:21:37.146322Z [info     ] document_completion_start      caller=\"src/llm/completions.py:42\" company=AMD document_type=management_discussion function=process_document_completion year=2020\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-29T08:21:37.843464Z [info     ] sending_chat_request           caller=\"src/llm/client.py:68\" function=get_chat_response input_tokens=7960 model=qwen3:4b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (k, document) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n\u001b[32m     22\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mdocument_loop\u001b[39m\u001b[33m\"\u001b[39m, document=document, index=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         result = \u001b[43mprocess_document_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_model_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m         completions_by_type[document.document_type].append(result)\n\u001b[32m     26\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mget_aggregations_for_document_types\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/completions.py:43\u001b[39m, in \u001b[36mprocess_document_completion\u001b[39m\u001b[34m(document, prompt, model_config)\u001b[39m\n\u001b[32m     40\u001b[39m messages = format_document_messages(prompt, document)\n\u001b[32m     42\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mdocument_completion_start\u001b[39m\u001b[33m\"\u001b[39m, company=company.ticker, year=filing.fiscal_year, document_type=document.document_type.value)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m response = \u001b[43mget_chat_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Extract necessary details from the response\u001b[39;00m\n\u001b[32m     46\u001b[39m created_at = response.created_at\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/client.py:82\u001b[39m, in \u001b[36mget_chat_response\u001b[39m\u001b[34m(model_config, messages, client)\u001b[39m\n\u001b[32m     71\u001b[39m     logger.warn(\u001b[33m\"\u001b[39m\u001b[33moversized_input\u001b[39m\u001b[33m\"\u001b[39m, input_tokens=input_tokens, num_ctx=model_config.num_ctx)\n\u001b[32m     73\u001b[39m options = {\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_ctx\u001b[39m\u001b[33m\"\u001b[39m: model_config.num_ctx,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: model_config.temperature,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_predict\u001b[39m\u001b[33m\"\u001b[39m: model_config.num_predict,\n\u001b[32m     80\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m response = \u001b[43mretry_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# response = client.chat(model_config.name,\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#     options = {\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m#         \"num_ctx\": model_config.num_ctx,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     93\u001b[39m \u001b[38;5;66;03m#     messages = messages,\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     95\u001b[39m duration=response.total_duration / \u001b[32m1e9\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/client.py:34\u001b[39m, in \u001b[36mretry_backoff\u001b[39m\u001b[34m(timeout, func, *args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m time.time() - start < timeout:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/ollama/_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/ollama/_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/ollama/_client.py:118\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    117\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     r.raise_for_status()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for (i, ticker) in enumerate(TICKERS):\n",
    "    # break\n",
    "    logger.info(\"get_completion_for_filing_documents\", company=company.name, ticker=company.ticker, index=f\"{i}/{len(TICKERS)}\")\n",
    "    LOOP_START = time.time()\n",
    "    company = get_company_by_ticker(ticker)\n",
    "    filings = get_filings_by_company(company.id)\n",
    "    # filings = [filing for filing in filings if filing.period_of_report.year > 2023]\n",
    "\n",
    "    # store output to make aggregate from\n",
    "    completions_by_type: defaultdict[DocumentType, List[Completion]] = defaultdict(list)\n",
    "\n",
    "    logger.debug(\"document_prompt\", content=f\"'{document_prompt.content.strip()[:90]}...'\")\n",
    "\n",
    "    for (j, filing) in enumerate(filings):\n",
    "        logger.info(\"filing_loop\", filing=filing, index=f\"{j}/{len(filings)}\")\n",
    "        documents = get_documents_by_filing(filing.id)\n",
    "        if len(documents) < 1:\n",
    "            logger.warning(\"no documents\")\n",
    "            continue\n",
    "\n",
    "        for (k, document) in enumerate(documents):\n",
    "            logger.info(\"document_loop\", document=document, index=f\"{k}/{len(filings)}\")\n",
    "            result = process_document_completion(document, document_prompt, document_model_config)\n",
    "            completions_by_type[document.document_type].append(result)\n",
    "\n",
    "    logger.info(\"get_aggregations_for_document_types\")\n",
    "    logger.debug(\"aggregate_prompt\", content=f\"'{aggregate_prompt.content.strip()[:90]}...'\")\n",
    "\n",
    "    logger.info(\"get_mda_aggregate\")\n",
    "    mda_aggregate = create_document_type_aggregate(DocumentType.MDA, completions_by_type[DocumentType.MDA], aggregate_prompt, aggregate_model_config)\n",
    "\n",
    "    logger.info(\"get_business_description_aggregate\")\n",
    "    description_aggregate = create_document_type_aggregate(DocumentType.DESCRIPTION, completions_by_type[DocumentType.DESCRIPTION], aggregate_prompt, aggregate_model_config)\n",
    "\n",
    "    # use a different prompt for risk factors\n",
    "    logger.info(\"get_risk_factors_aggregate\")\n",
    "    logger.info(\"risk_aggregate_prompt\", content=f\"'{risk_aggregate_prompt.content.strip()[:90]}...'\")\n",
    "    risk_aggregate = create_document_type_aggregate(DocumentType.RISK_FACTORS, completions_by_type[DocumentType.RISK_FACTORS], risk_aggregate_prompt, aggregate_model_config)\n",
    "\n",
    "\n",
    "    # Make the company 'front page summary' text\n",
    "    # remove <thinking>, format markdown, lint for misclanious wrapper text\n",
    "    logger.info(\"formatting_content\")\n",
    "    unformatted_markdown = remove_thinking_tags(description_aggregate.content)\n",
    "    with open('unformatted_markdown.md', 'w') as f:\n",
    "        f.write(unformatted_markdown)\n",
    "\n",
    "    markdown_response = get_generate_response(markdown_model_config, markdown_prompt.content, unformatted_markdown)\n",
    "    with open('formatted_markdown.md', 'w') as f:\n",
    "        f.write(markdown_response.response)\n",
    "\n",
    "    summary_response = get_generate_response(front_page_model_config, front_page_prompt.content, markdown_response.response)\n",
    "    linted_summary = get_generate_response(markdown_model_config, \"We are cleaning up for publication. Remove any introductory 'meta' lines from the provided document.\", summary_response.response)\n",
    "    company = update_company(company.id, {'summary': linted_summary.response})\n",
    "\n",
    "    LOOP_END = time.time()\n",
    "    logger.info(\"finished_company\", duration=f\"{LOOP_END - LOOP_START:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32388f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # regenerate an aggregate\n",
    "# from src.database.aggregates import Aggregate\n",
    "# from src.llm.prompts import format_aggregate_messages\n",
    "\n",
    "# # Get Aggregates where `aggregate.company.id == company.id` and `aggregate.document_type == document_type`\n",
    "# company = get_company_by_ticker(COMPANY)\n",
    "# document_type = DocumentType.DESCRIPTION\n",
    "\n",
    "# aggregates = (\n",
    "#     session.query(Aggregate)\n",
    "#     .filter(Aggregate.company_id == company.id)\n",
    "#     .filter(Aggregate.document_type == document_type)\n",
    "#     .all()\n",
    "# )\n",
    "\n",
    "# logger.info(\"retrieved_aggregates\", company=company.ticker, document_type=document_type.value, count=len(aggregates))\n",
    "# aggregate = max(aggregates, key=lambda agg: agg.created_at)\n",
    "# logger.info(\"most_recent_aggregate\", aggregate_id=aggregate.id, created_at=str(aggregate.created_at))\n",
    "# completions = aggregate.source_completions\n",
    "# logger.info(\"comlpetions\", length=len(completions))\n",
    "\n",
    "# # check the formatting of context\n",
    "# # formatted_context = format_aggregate_messages(aggregate_prompt, completions)\n",
    "# # print(formatted_context[1]['content'])\n",
    "\n",
    "# logger.info(\"aggregate_prompt\", content=f\"'{aggregate_prompt.content.strip()[:80]}...'\")\n",
    "# new_agg = create_document_type_aggregate(document_type, completions, aggregate_prompt, aggregate_model_config)\n",
    "# new_agg.content = remove_thinking_tags(new_agg.content)\n",
    "# with open('unformatted_markdown.md', 'w') as f:\n",
    "#     f.write(new_agg.content)\n",
    "\n",
    "# logger.info(\"formatting_markdown\")\n",
    "# new_agg = get_generate_response(markdown_model_config, markdown_prompt.content, new_agg.content)\n",
    "# new_agg = get_generate_response(markdown_model_config, \"We are cleaning up for publication. Remove any introductory 'meta' lines from the provided document.\", new_agg.response)\n",
    "# new_agg = get_generate_response(markdown_model_config, \"This document will be presented with a separate heading. Trim \", new_agg.response)\n",
    "# with open('formatted_markdown.md', 'w') as f:\n",
    "#     f.write(new_agg.response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAL', 'AAPL', 'ACHR', 'AES', 'AGNC', 'AMD', 'AMZN', 'APLD', 'AUR', 'AVGO', 'BA', 'BAC', 'CCL', 'CDE', 'CLF', 'CLSK', 'CSCO', 'CVE', 'F', 'GME', 'GOOGL', 'HBANP', 'HL', 'HOOD', 'HPE', 'INTC', 'IONQ', 'IPG', 'JOBY', 'KGC', 'KVUE', 'LCID', 'MARA', 'MP', 'MSFT', 'MU', 'NFLX', 'NU', 'NVDA', 'ORCL', 'PCG', 'PFE', 'PLTR', 'PR', 'QUBT', 'RIG', 'RIOT', 'RKLB', 'RKT', 'RXRX', 'S', 'SMCI', 'SMR', 'SNAP', 'SOFI', 'T', 'TSLA', 'UBER', 'UNH', 'VZ', 'WBD', 'WFC', 'WMT', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "from src.database.companies import get_all_company_tickers\n",
    "\n",
    "companies = get_all_company_tickers()\n",
    "print(companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = filings[0].documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71093f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_description"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.document_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4bae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMD 2020 10-K business_description"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630372bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD 2020 10-K business_description\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b337a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
