{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d7aae8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-20T10:16:48.425579Z [info     ] database_initialized           status=success\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:16:48.465904Z [info     ] retrieved_company_by_ticker    company=ADVANCED MICRO DEVICES INC ticker=AMD\n",
      "2025-07-20T10:16:48.467495Z [info     ] retrieved_filings_by_company   count=5\n",
      "2025-07-20T10:16:48.467832Z [info     ] filtered_list_of_filings       filings=[AMD 2020 10-K, AMD 2021 10-K, AMD 2022 10-K, AMD 2023 10-K, AMD 2024 10-K]\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:17:40.393745Z [info     ] role='assistant' content='Okay, let\\'s break down `go while` (which should be `go while` in Go, as Go doesn\\'t directly support \"while\" as a keyword, but uses `for` loops for the same purpose) and how to use it effectively.  I\\'ll provide explanations, examples, and common patterns.\\n\\n**Understanding `for` as `while` in Go**\\n\\nGo doesn\\'t have a `while` loop construct like some other languages (e.g., C, Python, Java).  Instead, it uses the `for` loop to achieve the same functionality.  The general structure is:\\n\\n```go\\nfor condition {\\n    // Code to be executed as long as the condition is true\\n}\\n```\\n\\nThis looks and behaves exactly like a `while` loop in many other languages.  The `condition` is evaluated before each iteration, and if it\\'s `true`, the code inside the loop is executed.\\n\\n**Examples**\\n\\nLet\\'s illustrate with several examples.\\n\\n1. **Simple Counter**\\n\\n   ```go\\n   package main\\n\\n   import \"fmt\"\\n\\n   func main() {\\n       count := 0\\n       for count < 5 {\\n           fmt.Println(\"Count:\", count)\\n           count++ // Increment the counter (essential to avoid an infinite loop)\\n       }\\n   }\\n   ```\\n\\n   *Explanation:*\\n\\n   * `count := 0`:  Initializes a variable `count` to 0.\\n   * `for count < 5 { ... }`: This is our \"while\" loop. It continues as long as `count` is less than 5.\\n   * `fmt.Println(\"Count:\", count)`:  Prints the current value of `count`.\\n   * `count++`:  Increments `count` by 1 after each iteration.  Without this, the condition `count < 5` would always be true, and the loop would run forever (an infinite loop).\\n\\n2. **Looping Until a Condition is Met**\\n\\n   ```go\\n   package main\\n\\n   import \"fmt\"\\n\\n   func main() {\\n       userInput := \"\"\\n       for userInput != \"quit\" {\\n           fmt.Print(\"Enter a command (or \\'quit\\' to exit): \")\\n           fmt.Scanln(&userInput)  // Reads user input\\n           fmt.Println(\"You entered:\", userInput)\\n       }\\n       fmt.Println(\"Exiting.\")\\n   }\\n   ```\\n\\n   *Explanation:*\\n\\n   * `userInput := \"\"`: Initializes an empty string for user input.\\n   * `for userInput != \"quit\" { ... }`: The loop continues as long as the user doesn\\'t enter \"quit\".\\n   * `fmt.Print(...)`:  Prompts the user to enter a command.\\n   * `fmt.Scanln(&userInput)`: Reads the user\\'s input and stores it in the `userInput` variable.\\n   * `fmt.Println(...)`: Prints the user\\'s input.\\n\\n3. **Iterating Over a Slice/Array (Another Common Use)**\\n\\n   Although this isn\\'t technically a \"while\" loop in the classic sense, often you\\'re doing something that *feels* like a `while` loop when iterating over a data structure.\\n\\n   ```go\\n   package main\\n\\n   import \"fmt\"\\n\\n   func main() {\\n       numbers := []int{1, 2, 3, 30, 4, 5}\\n\\n       i := 0\\n       for i < len(numbers) {\\n           fmt.Println(\"Number at index\", i, \":\", numbers[i])\\n           i++\\n       }\\n   }\\n   ```\\n\\n   *Explanation:*\\n\\n   * `numbers := []int{1, 2, 3, 30, 4, 5}`: Creates a slice of integers.\\n   * `i := 0`:  Initializes an index `i` to 0.\\n   * `for i < len(numbers) { ... }`: The loop continues as long as the index `i` is less than the length of the slice.  This is a standard way to iterate through a slice.\\n   * `fmt.Println(...)`: Prints the number at the current index.\\n   * `i++`:  Increments the index.\\n\\n**Key Considerations and Potential Pitfalls**\\n\\n* **Infinite Loops:**  The most common mistake is forgetting to update the condition within the loop.  If the condition never becomes false, the loop will run forever.  Make sure something inside the loop modifies the variables involved in the condition.\\n\\n* **Condition Evaluation:**  The condition in the `for` loop is evaluated *before* each iteration.  If the condition is initially `false`, the loop body will not be executed at all.\\n\\n* **Clarity:** While `for` loops can effectively emulate `while` loops, if the logic is complex, sometimes it can improve readability to use a separate boolean variable to control the loop:\\n\\n   ```go\\n   package main\\n\\n   import \"fmt\"\\n\\n   func main() {\\n       keepGoing := true\\n       count := 0\\n       for keepGoing {\\n           fmt.Println(\"Count:\", count)\\n           count++\\n           if count >= 10 {\\n               keepGoing = false\\n           }\\n       }\\n   }\\n   ```\\n   This can be easier to understand in some situations, as it separates the loop control variable from the main logic.\\n\\n**Complete Example with Explanation**\\n\\n```go\\npackage main\\n\\nimport \"fmt\"\\n\\nfunc main() {\\n    // 1. Initialize a variable that will control the loop\\n    value := 10\\n\\n    // 2. Start the \"while\" loop (using Go\\'s \\'for\\' loop syntax)\\n    for value > 0 {\\n        // 3. Code to be executed in each iteration\\n        fmt.Println(\"Value:\", value)\\n\\n        // 4. Update the value to eventually make the condition false\\n        value--  // Decrement the value by 1\\n    }\\n\\n    // 5. Code after the loop finishes\\n    fmt.Println(\"Loop finished.\")\\n}\\n```\\n\\n*   **`value := 10`**: We start with a variable named `value` initialized to 10. This variable will be the basis for our loop\\'s condition.\\n*   **`for value > 0 { ... }`**:  This is our \"while\" loop.  The code inside the curly braces `{}` will be executed repeatedly as long as the condition `value > 0` is true.\\n*   **`fmt.Println(\"Value:\", value)`**: This line prints the current value of the `value` variable to the console.\\n*   **`value--`**: This line is crucial. It decrements the `value` variable by 1. Without this line, the condition `value > 0` would *always* be true, and the loop would run forever (an infinite loop).\\n*   **`fmt.Println(\"Loop finished.\")`**:  This line is executed *after* the loop condition `value > 0` becomes false (i.e., when `value` becomes 0).\\n\\n**How to Ask Further Questions**\\n\\nWhen asking about `go while` or other Go looping constructs, please provide:\\n\\n*   **The specific problem you\\'re trying to solve:**  What are you trying to achieve with the loop?\\n*   **The data you\\'re working with:**  Is it a slice, a map, a file, etc.?\\n*   **The relevant code you\\'re struggling with:**  Paste your code (if possible).\\n*   **The error message (if any):**  Copy and paste the error message you\\'re seeing.  This is critical for diagnosis.\\n' images=None tool_calls=None\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:17:45.471421Z [info     ] Okay, here's a one-liner joke tailored for /r/unixporn, aiming for that dry, witty, engineer-friendly vibe:\n",
      "\n",
      "**\"My dotfiles are more documented than my last performance review.\"**\n",
      "\n",
      "---\n",
      "\n",
      "**Why it *should* work:**\n",
      "\n",
      "*   **Relatable to the Audience:** Engineers and young professionals are often familiar with poorly documented performance reviews.\n",
      "*   **Unix/Dev Humor:** References the dedication (often bordering on obsession) many /r/unixporn users have for their dotfiles.\n",
      "*   **Dry & Understated:** It's not a *laugh-out-loud* joke, but a wry observation.\n",
      "*   **Slightly Self-Deprecating:**  It's poking fun at a common frustration.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you’d like a few more variations!\n"
     ]
    }
   ],
   "source": [
    "# Imports and Config\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from src.database.base import init_db\n",
    "from src.database.aggregates import get_aggregate\n",
    "from src.database.filings import get_filings_by_company\n",
    "from src.database.companies import get_company_by_ticker, update_company\n",
    "from src.database.completions import Completion\n",
    "from src.database.documents import get_documents_by_filing, DocumentType\n",
    "from src.database.prompts import *\n",
    "\n",
    "from src.llm.client import ModelConfig, init_client, count_tokens, remove_thinking_tags, get_generate_response\n",
    "from src.llm.aggregations import create_document_type_aggregate\n",
    "from src.llm.completions import process_document_completion\n",
    "\n",
    "from src.utils.config import settings\n",
    "from src.utils.logging import configure_logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "init_db(settings.database.url)\n",
    "session = get_db_session()\n",
    "\n",
    "client = init_client(settings.openai_api.url)\n",
    "\n",
    "COMPANY = \"AMD\"\n",
    "TICKERS = [\"AMD\", \"AAPL\", \"GOOGL\", ]\n",
    "company = get_company_by_ticker(COMPANY)\n",
    "filings = get_filings_by_company(company.id)\n",
    "# filings = [filing for filing in filings if filing.period_of_report.year > 2023]\n",
    "logger.info(\"filtered_list_of_filings\", filings=filings)\n",
    "\n",
    "from src.llm.client import retry_backoff\n",
    "\n",
    "response = retry_backoff(3600, client.chat, 'gemma3:12b', options={\"temperature\": 1.0}, messages=[{\"role\": \"user\", \"content\": \"go while-d\"}])\n",
    "logger.info(response.message)\n",
    "\n",
    "response = retry_backoff(\n",
    "    timeout=3,\n",
    "    func=client.generate,\n",
    "    model = \"gemma3:12b\",\n",
    "    system = \"you're an intelligent comedian who finds humor in witty dry comedy. Your target audience is young professionals and engineers.\",\n",
    "    prompt = \"Write a one line joke for the front page of /r/unixporn\",\n",
    "    options = {\n",
    "    },\n",
    ")\n",
    "logger.info(response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a26852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new prompts\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"focused_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#         We're archivists! We need to transform this input into output that is concise, factual, sometimes said with a hint of personality.  Our purpose is to accurately represent the information contained in the input text.  We prioritize completeness; ensure every relevant point is captured, even if seemingly minor. We look for literal and implied meanings.\n",
    "#         \"\"\",\n",
    "# }\n",
    "# document_prompt = create_prompt(data)\n",
    "\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"aggregate_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#     You are a content writer with a worldwide audience of talented young professionals.\n",
    "#     You will recieve a large series of research notes, sometimes spanning months or years.\n",
    "#     Take these notes and write a summary of the topics discuessed therein.\n",
    "\n",
    "#     Use markdown to format your response:\n",
    "#     - list items\n",
    "#     - **bold text**\n",
    "#     - _italics_\n",
    "#     - Headings require one or more '#' characters\n",
    "# \"\"\",\n",
    "# }\n",
    "# aggregate_prompt = create_prompt(data)\n",
    "\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"aggregate_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#     You're a change analyst! You will be presented with a series of documents. Each document has a meta section with important context (including a date).\n",
    "\n",
    "#     Compare each of the documents.\n",
    "\n",
    "#     Summarize the progression of each document.\n",
    "\n",
    "#     Use markdown to format your response:\n",
    "#     - list items\n",
    "#     - **bold text**\n",
    "#     - _italics_\n",
    "#     - Headings require one or more '#' characters\n",
    "# \"\"\",\n",
    "# }\n",
    "# risk_aggregate_prompt = create_prompt(data)\n",
    "\n",
    "\n",
    "# create markdown prompt\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"markdown_sanitation\",\n",
    "#     \"content\": \"\"\"\n",
    "# You are a formatting wizard who's job it is to enforce proper markdown page structure!\n",
    "\n",
    "# You MUST NOT change any words or phrases in the provided document.\n",
    "\n",
    "# You also MUST NOT add any form of wrapping or text around the document (backticks, quotes, etc)\n",
    "\n",
    "# One common formatting error is the use of **bolded text** in place of a subheading (#'s).\n",
    "# Here is an example of this error:\n",
    "# <error>\n",
    "# **Palantir: A Story of Innovation, Ethics, and Global Impact**\n",
    "\n",
    "# In 2003, a group of technologists, entrepreneurs, and former intelligence officers founded Palantir with a bold mission: to build software that could transform how the world handles data, decisions, and operations. What began as a tool for counterterrorism in the U.S. intelligence community evolved into a global leader in data integration, AI, and ethical technology. Today, Palantir’s platforms empower governments, corporations, and institutions to solve complex problems—from combating crime to advancing public health—while upholding rigorous privacy and ethical standards.\n",
    "\n",
    "# **The Power of Platforms**\n",
    "# At the heart of Palantir’s success are its four core platforms, each designed to tackle unique challenges:\n",
    "# - **Gotham** serves as a central nervous system for intelligence and defense agencies, synthesizing vast datasets (e.g., signals intelligence, financial records) to uncover patterns and guide high-stakes decisions.\n",
    "# - **Foundry** acts as a universal operating system for data, enabling organizations to build, manage, and optimize data pipelines across departments, from data engineers to executives.\n",
    "# - **Apollo** ensures seamless, secure deployment of software across any cloud or on-premise environment, minimizing downtime and maximizing security.\n",
    "# - **AIP (Artificial Intelligence Platform)**, a recent innovation, integrates cutting-edge machine learning and generative AI (e.g., large language models) with Palantir’s existing systems, allowing enterprises to operationalize AI in real-time, from fraud detection to predictive analytics.\n",
    "\n",
    "# **Customers, Revenue, and Global Reach**\n",
    "# Palantir’s customer base spans 711 organizations as of 2024, with 55% of revenue coming from government clients (U.S. federal agencies, international defense, and public health) and 45% from commercial sectors like healthcare, energy, and finance. Their U.S. operations drive 66% of revenue, with the remaining 34% generated internationally. Top customers, such as the U.S. military and major financial institutions, see average annual revenue from Palantir exceeding $64 million—a testament to the scalability of their platforms.\n",
    "\n",
    "# **A Sales Strategy Built on Trust**\n",
    "# Palantir targets large-scale, mission-critical projects, leveraging a direct sales force, strategic partnerships (e.g., with Fujitsu), and cloud providers to expand access. Their \"Developer Tier\" initiative democratizes access to Foundry and AIP, allowing developers in the U.S. and select countries to experiment with their tools. This approach, combined with bootcamps and training, has helped Palantir navigate the risk-averse culture of government clients, where rapid deployment and proven outcomes are paramount.\n",
    "\n",
    "# **Privacy: The Bedrock of Innovation**\n",
    "# From the start, Palantir has prioritized privacy as a non-negotiable feature. Their \"privacy by design\" philosophy embeds data minimization, dynamic access controls, and auditability into every platform. For example, Foundry ensures compliance with GDPR and HIPAA through automated data retention policies and granular permissions. This commitment extends to AIP, where human oversight—rather than algorithmic automation—guides critical decisions. Palantir’s ethical framework, which includes accountability and transparency, has become a differentiator in an era of growing AI scrutiny.\n",
    "\n",
    "# **Innovation at the Edge**\n",
    "# Research and development drive Palantir’s evolution. Teams work on edge computing, AI integration, and user-centric design, often collaborating directly with customers to refine solutions. AIP bootcamps, for instance, enable real-world testing of AI applications, from healthcare diagnostics to supply chain optimization. These efforts ensure that Palantir’s tools remain at the forefront of technological progress, even as competitors—both traditional software giants and AI startups—strive to replicate their success.\n",
    "\n",
    "# **Global Impact, Local Roots**\n",
    "# Palantir’s platforms have transformed industries:\n",
    "# - **Law Enforcement**: Streamlined investigations and resource allocation.\n",
    "# - **Healthcare**: Enhanced disease tracking and drug discovery.\n",
    "# - **Finance**: Strengthened fraud detection and compliance.\n",
    "# With 3,936 employees globally (31% outside the U.S.), Palantir balances global ambition with local engagement, fostering inclusive workplaces and adhering to labor standards, including France’s works council requirements.\n",
    "\n",
    "# **Challenges and the Road Ahead**\n",
    "# Despite its strengths, Palantir faces hurdles: competition from internal development teams, the need to balance AI innovation with ethical boundaries, and the seasonal nature of enterprise software sales. Yet, its focus on privacy, scalability, and mission-driven partnerships positions it to navigate these challenges. As AIP and Foundry continue to evolve, Palantir remains committed to its founding vision: using technology not just to solve problems, but to do so responsibly,\n",
    "# </error>\n",
    "\n",
    "# And the corrected formatting:\n",
    "\n",
    "# <correct>\n",
    "# # Palantir: A Story of Innovation, Ethics, and Global Impact**\n",
    "\n",
    "# In 2003, a group of technologists, entrepreneurs, and former intelligence officers founded Palantir with a bold mission: to build software that could transform how the world handles data, decisions, and operations. What began as a tool for counterterrorism in the U.S. intelligence community evolved into a global leader in data integration, AI, and ethical technology. Today, Palantir’s platforms empower governments, corporations, and institutions to solve complex problems—from combating crime to advancing public health—while upholding rigorous privacy and ethical standards.\n",
    "\n",
    "# ## The Power of Platforms\n",
    "# At the heart of Palantir’s success are its four core platforms, each designed to tackle unique challenges:\n",
    "# - **Gotham** serves as a central nervous system for intelligence and defense agencies, synthesizing vast datasets (e.g., signals intelligence, financial records) to uncover patterns and guide high-stakes decisions.\n",
    "# - **Foundry** acts as a universal operating system for data, enabling organizations to build, manage, and optimize data pipelines across departments, from data engineers to executives.\n",
    "# - **Apollo** ensures seamless, secure deployment of software across any cloud or on-premise environment, minimizing downtime and maximizing security.\n",
    "# - **AIP (Artificial Intelligence Platform)**, a recent innovation, integrates cutting-edge machine learning and generative AI (e.g., large language models) with Palantir’s existing systems, allowing enterprises to operationalize AI in real-time, from fraud detection to predictive analytics.\n",
    "\n",
    "# ### Customers, Revenue, and Global Reach\n",
    "# Palantir’s customer base spans 711 organizations as of 2024, with 55% of revenue coming from government clients (U.S. federal agencies, international defense, and public health) and 45% from commercial sectors like healthcare, energy, and finance. Their U.S. operations drive 66% of revenue, with the remaining 34% generated internationally. Top customers, such as the U.S. military and major financial institutions, see average annual revenue from Palantir exceeding $64 million—a testament to the scalability of their platforms.\n",
    "\n",
    "# ### A Sales Strategy Built on Trust\n",
    "# Palantir targets large-scale, mission-critical projects, leveraging a direct sales force, strategic partnerships (e.g., with Fujitsu), and cloud providers to expand access. Their \"Developer Tier\" initiative democratizes access to Foundry and AIP, allowing developers in the U.S. and select countries to experiment with their tools. This approach, combined with bootcamps and training, has helped Palantir navigate the risk-averse culture of government clients, where rapid deployment and proven outcomes are paramount.\n",
    "\n",
    "# ### Privacy: The Bedrock of Innovation\n",
    "# From the start, Palantir has prioritized privacy as a non-negotiable feature. Their \"privacy by design\" philosophy embeds data minimization, dynamic access controls, and auditability into every platform. For example, Foundry ensures compliance with GDPR and HIPAA through automated data retention policies and granular permissions. This commitment extends to AIP, where human oversight—rather than algorithmic automation—guides critical decisions. Palantir’s ethical framework, which includes accountability and transparency, has become a differentiator in an era of growing AI scrutiny.\n",
    "\n",
    "# ### Innovation at the Edge\n",
    "# Research and development drive Palantir’s evolution. Teams work on edge computing, AI integration, and user-centric design, often collaborating directly with customers to refine solutions. AIP bootcamps, for instance, enable real-world testing of AI applications, from healthcare diagnostics to supply chain optimization. These efforts ensure that Palantir’s tools remain at the forefront of technological progress, even as competitors—both traditional software giants and AI startups—strive to replicate their success.\n",
    "\n",
    "# ## Global Impact, Local Roots\n",
    "# Palantir’s platforms have transformed industries:\n",
    "# - **Law Enforcement**: Streamlined investigations and resource allocation.\n",
    "# - **Healthcare**: Enhanced disease tracking and drug discovery.\n",
    "# - **Finance**: Strengthened fraud detection and compliance.\n",
    "# With 3,936 employees globally (31% outside the U.S.), Palantir balances global ambition with local engagement, fostering inclusive workplaces and adhering to labor standards, including France’s works council requirements.\n",
    "\n",
    "# ## Challenges and the Road Ahead**\n",
    "# Despite its strengths, Palantir faces hurdles: competition from internal development teams, the need to balance AI innovation with ethical boundaries, and the seasonal nature of enterprise software sales. Yet, its focus on privacy, scalability, and mission-driven partnerships positions it to navigate these challenges. As AIP and Foundry continue to evolve, Palantir remains committed to its founding vision: using technology not just to solve problems, but to do so responsibly,\n",
    "# </correct>\n",
    "#         \"\"\",\n",
    "# }\n",
    "# markdown_prompt = create_prompt(data)\n",
    "\n",
    "# create summary prompt\n",
    "# i = len(get_prompt_ids()) + 1\n",
    "# data = {\n",
    "#     \"name\": f\"Prompt #{i}\",\n",
    "#     \"role\": PromptRole.SYSTEM,\n",
    "#     \"description\": \"front_page_summary\",\n",
    "#     \"content\": \"\"\"\n",
    "#         You're a fantastic ad copy editor. The user will provide you with a document, which you need to boil down into it's most concise esscense.\n",
    "\n",
    "#         Aim for no more than 50 words, 75 at the very most.\n",
    "\n",
    "#         Use markdown (**bold text**, _italics_, - bullet points) to add emphasis to your response.\n",
    "#     \"\"\",\n",
    "# }\n",
    "# front_page_prompt = create_prompt(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381e976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-20T10:17:45.490115Z [info     ] retrieved_prompt               prompt_id=06878708-7a86-76e0-8000-8b1f4e2ed08c\n",
      "2025-07-20T10:17:45.491171Z [info     ] retrieved_prompt               prompt_id=0687ab13-8e9b-7d45-8000-e36b40bd5fcd\n",
      "2025-07-20T10:17:45.492357Z [info     ] retrieved_prompt               prompt_id=0687abe9-4f99-7ca7-8000-77b422a5c1dc\n",
      "2025-07-20T10:17:45.493600Z [info     ] retrieved_prompt               prompt_id=0687a885-7cc1-7e88-8000-a95d86e69d22\n",
      "2025-07-20T10:17:45.494369Z [info     ] retrieved_prompt               prompt_id=0687a424-1757-743d-8000-01257e133b73\n"
     ]
    }
   ],
   "source": [
    "# prompts and model config\n",
    "document_prompt = get_prompt('06878708-7a86-76e0-8000-8b1f4e2ed08c')\n",
    "document_model_config = ModelConfig(\n",
    "    name=\"qwen3:4b\",\n",
    "    num_ctx=24567,\n",
    "    temperature=0.8,\n",
    "    top_k=45\n",
    ")\n",
    "\n",
    "# aggregate_prompt = get_prompt(\"068786e2-1e9e-7eb6-8000-e716f8eb8a3f\")\n",
    "# aggregate_prompt = get_prompt(\"0687a819-b5cb-73a3-8000-4326f7819960\")\n",
    "aggregate_prompt = get_prompt(\"0687ab13-8e9b-7d45-8000-e36b40bd5fcd\")\n",
    "\n",
    "risk_aggregate_prompt = get_prompt(\"0687abe9-4f99-7ca7-8000-77b422a5c1dc\")\n",
    "\n",
    "aggregate_model_config = ModelConfig(\n",
    "    name=\"qwen3:14b\",\n",
    "    num_ctx=9000,\n",
    "    temperature=0.7,\n",
    "    num_gpu=41\n",
    ")\n",
    "\n",
    "# aggregate_model_config = ModelConfig(\n",
    "#     name=\"hf.co/unsloth/gemma-3-12b-it-qat-GGUF:latest\",\n",
    "#     num_ctx=10000,\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "markdown_prompt = get_prompt('0687a885-7cc1-7e88-8000-a95d86e69d22')\n",
    "markdown_model_config = ModelConfig(\n",
    "    name=\"hf.co/unsloth/gemma-3-12b-it-qat-GGUF:latest\",\n",
    "    num_ctx=10000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "front_page_prompt = get_prompt('0687a424-1757-743d-8000-01257e133b73')\n",
    "front_page_model_config = ModelConfig(\n",
    "    name=\"hf.co/unsloth/gemma-3-12b-it-qat-GGUF:latest\",\n",
    "    num_ctx=10000,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc26195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-20T10:17:45.504643Z [info     ] get_completion_for_filing_documents company=ADVANCED MICRO DEVICES INC index=0/3 ticker=AMD\n",
      "2025-07-20T10:17:45.506786Z [info     ] retrieved_company_by_ticker    company=ADVANCED MICRO DEVICES INC ticker=AMD\n",
      "2025-07-20T10:17:45.508120Z [info     ] retrieved_filings_by_company   count=5\n",
      "2025-07-20T10:17:45.508511Z [info     ] filing_loop                    filing=AMD 2020 10-K index=0/5\n",
      "2025-07-20T10:17:45.511196Z [info     ] retrieved_documents_by_filing  count=3 filing_id=0685e25b-10ea-779d-8000-cb13053c2e9e\n",
      "2025-07-20T10:17:45.512863Z [info     ] document_loop                  document=AMD 2020 10-K business_description index=0/5\n",
      "2025-07-20T10:17:45.513819Z [info     ] document_completion_start      company=AMD document_type=business_description year=2020\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:17:46.321685Z [info     ] sending_chat_request           input_tokens=12224 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:19:00.876313Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1715 tokens_per_second=23.12 tokens/s\n",
      "2025-07-20T10:19:00.889817Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc29-4e18-7dc4-8000-3b20b2496a5e\n",
      "2025-07-20T10:19:00.891082Z [info     ] document_loop                  document=AMD 2020 10-K risk_factors index=1/5\n",
      "2025-07-20T10:19:00.893906Z [info     ] document_completion_start      company=AMD document_type=risk_factors year=2020\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:19:01.629219Z [info     ] sending_chat_request           input_tokens=19562 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:20:48.791114Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1629 tokens_per_second=15.25 tokens/s\n",
      "2025-07-20T10:20:48.808160Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc30-0cb2-7ca6-8000-ed9b803c890c\n",
      "2025-07-20T10:20:48.809319Z [info     ] document_loop                  document=AMD 2020 10-K management_discussion index=2/5\n",
      "2025-07-20T10:20:48.811455Z [info     ] document_completion_start      company=AMD document_type=management_discussion year=2020\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:20:49.530938Z [info     ] sending_chat_request           input_tokens=7960 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:21:34.205142Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1657 tokens_per_second=37.40 tokens/s\n",
      "2025-07-20T10:21:34.214606Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc32-e351-77a9-8000-8192864d1e86\n",
      "2025-07-20T10:21:34.215547Z [info     ] filing_loop                    filing=AMD 2021 10-K index=1/5\n",
      "2025-07-20T10:21:34.218087Z [info     ] retrieved_documents_by_filing  count=3 filing_id=0685e25b-7555-7f3c-8000-29403a92a066\n",
      "2025-07-20T10:21:34.218467Z [info     ] document_loop                  document=AMD 2021 10-K business_description index=0/5\n",
      "2025-07-20T10:21:34.219324Z [info     ] document_completion_start      company=AMD document_type=business_description year=2021\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:21:34.939699Z [info     ] sending_chat_request           input_tokens=10401 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:22:22.419951Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1263 tokens_per_second=26.84 tokens/s\n",
      "2025-07-20T10:22:22.430851Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc35-e6c3-7855-8000-0af83ae10cae\n",
      "2025-07-20T10:22:22.431923Z [info     ] document_loop                  document=AMD 2021 10-K risk_factors index=1/5\n",
      "2025-07-20T10:22:22.434364Z [info     ] document_completion_start      company=AMD document_type=risk_factors year=2021\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:22:23.197319Z [info     ] sending_chat_request           input_tokens=19883 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:23:59.307397Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1377 tokens_per_second=14.38 tokens/s\n",
      "2025-07-20T10:23:59.317599Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc3b-f4f6-761f-8000-b995d31c576d\n",
      "2025-07-20T10:23:59.318652Z [info     ] document_loop                  document=AMD 2021 10-K management_discussion index=2/5\n",
      "2025-07-20T10:23:59.320416Z [info     ] document_completion_start      company=AMD document_type=management_discussion year=2021\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:24:00.009150Z [info     ] sending_chat_request           input_tokens=6839 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:24:44.110493Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1840 tokens_per_second=42.08 tokens/s\n",
      "2025-07-20T10:24:44.120497Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc3e-c1ce-72ec-8000-3f68cc2a2872\n",
      "2025-07-20T10:24:44.121489Z [info     ] filing_loop                    filing=AMD 2022 10-K index=2/5\n",
      "2025-07-20T10:24:44.124505Z [info     ] retrieved_documents_by_filing  count=3 filing_id=0685e25b-d0a4-78ee-8000-4062edd6828d\n",
      "2025-07-20T10:24:44.124828Z [info     ] document_loop                  document=AMD 2022 10-K business_description index=0/5\n",
      "2025-07-20T10:24:44.125626Z [info     ] document_completion_start      company=AMD document_type=business_description year=2022\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:24:44.852017Z [info     ] sending_chat_request           input_tokens=10603 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:25:33.779991Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1280 tokens_per_second=26.35 tokens/s\n",
      "2025-07-20T10:25:33.784632Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc41-dc84-7341-8000-d29dbcaf8579\n",
      "2025-07-20T10:25:33.785463Z [info     ] document_loop                  document=AMD 2022 10-K risk_factors index=1/5\n",
      "2025-07-20T10:25:33.787769Z [info     ] document_completion_start      company=AMD document_type=risk_factors year=2022\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:25:34.521298Z [info     ] sending_chat_request           input_tokens=20556 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:27:02.182472Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1138 tokens_per_second=13.04 tokens/s\n",
      "2025-07-20T10:27:02.187733Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc47-62f7-7354-8000-79d222b177ff\n",
      "2025-07-20T10:27:02.188868Z [info     ] document_loop                  document=AMD 2022 10-K management_discussion index=2/5\n",
      "2025-07-20T10:27:02.190704Z [info     ] document_completion_start      company=AMD document_type=management_discussion year=2022\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:27:02.899385Z [info     ] sending_chat_request           input_tokens=9090 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:27:48.435511Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1447 tokens_per_second=32.03 tokens/s\n",
      "2025-07-20T10:27:48.445834Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc4a-4703-77a9-8000-d196579bdc46\n",
      "2025-07-20T10:27:48.446656Z [info     ] filing_loop                    filing=AMD 2023 10-K index=3/5\n",
      "2025-07-20T10:27:48.448924Z [info     ] retrieved_documents_by_filing  count=3 filing_id=0685e25c-3b7b-70d8-8000-0e258b2942fa\n",
      "2025-07-20T10:27:48.449282Z [info     ] document_loop                  document=AMD 2023 10-K business_description index=0/5\n",
      "2025-07-20T10:27:48.449893Z [info     ] document_completion_start      company=AMD document_type=business_description year=2023\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:27:49.158827Z [info     ] sending_chat_request           input_tokens=12482 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:28:52.924360Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1438 tokens_per_second=22.72 tokens/s\n",
      "2025-07-20T10:28:52.929094Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc4e-4ed3-7be2-8000-cfd1f4cf247c\n",
      "2025-07-20T10:28:52.930256Z [info     ] document_loop                  document=AMD 2023 10-K risk_factors index=1/5\n",
      "2025-07-20T10:28:52.932953Z [info     ] document_completion_start      company=AMD document_type=risk_factors year=2023\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:28:53.671006Z [info     ] sending_chat_request           input_tokens=22258 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:30:33.871567Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1218 tokens_per_second=12.20 tokens/s\n",
      "2025-07-20T10:30:33.881567Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc54-9dfd-7dd8-8000-35b5dae8b6f4\n",
      "2025-07-20T10:30:33.882899Z [info     ] document_loop                  document=AMD 2023 10-K management_discussion index=2/5\n",
      "2025-07-20T10:30:33.884779Z [info     ] document_completion_start      company=AMD document_type=management_discussion year=2023\n",
      "HTTP Request: GET http://10.0.0.4:8000/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:30:34.616506Z [info     ] sending_chat_request           input_tokens=7859 model=qwen3:4b\n",
      "HTTP Request: POST http://10.0.0.4:8000/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-07-20T10:31:13.187031Z [info     ] recieved_chat_response         done=True done_reason=stop duration=0.00s model=qwen3:4b output_tokens=1429 tokens_per_second=37.40 tokens/s\n",
      "2025-07-20T10:31:13.191731Z [info     ] create_completion              __name__=src.database.completions completion_id=0687cc57-1308-72f1-8000-5f2369d033c7\n",
      "2025-07-20T10:31:13.192737Z [info     ] filing_loop                    filing=AMD 2024 10-K index=4/5\n",
      "2025-07-20T10:31:13.195168Z [info     ] retrieved_documents_by_filing  count=3 filing_id=0685e25c-88d4-7ff4-8000-2d56bd5cc2bf\n",
      "2025-07-20T10:31:13.195475Z [info     ] document_loop                  document=AMD 2024 10-K business_description index=0/5\n",
      "2025-07-20T10:31:13.196323Z [info     ] document_completion_start      company=AMD document_type=business_description year=2024\n",
      "2025-07-20T10:31:13.207405Z [error    ] retry_backoff                  backoff=1 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:31:15.208688Z [error    ] retry_backoff                  backoff=2 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:31:19.209987Z [error    ] retry_backoff                  backoff=4 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:31:27.211362Z [error    ] retry_backoff                  backoff=8 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:31:43.212898Z [error    ] retry_backoff                  backoff=16 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:32:15.219680Z [error    ] retry_backoff                  backoff=32 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:33:18.305562Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:34:21.409425Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:35:24.513437Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:36:27.617424Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:37:30.722310Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:38:33.825403Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:39:36.929361Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:40:40.033375Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:41:43.137469Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:42:46.241419Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:43:49.345476Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:44:52.450290Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:45:55.553342Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:46:58.657445Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:48:01.761373Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:49:04.865480Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:50:07.969473Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:51:11.073452Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:52:14.177375Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:53:17.281441Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:54:20.385691Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:55:23.489323Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:56:26.593721Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:57:29.697936Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:58:32.801371Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T10:59:35.905332Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:00:39.009358Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:01:42.113421Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:02:45.218305Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:03:48.321372Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:04:51.425347Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:05:54.529347Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:06:57.633406Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:08:00.739485Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:09:03.841342Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:10:06.945358Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:11:10.049422Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:12:13.153364Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:13:16.257370Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:14:19.361442Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:15:22.465368Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:16:25.569487Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:17:28.673404Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:18:31.777405Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:19:34.881347Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:20:37.985483Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:21:41.089428Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:22:44.193918Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:23:47.297351Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:24:50.402327Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:25:53.505362Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:26:56.609363Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:27:59.713351Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:29:02.817422Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:30:05.921511Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:31:09.025387Z [error    ] retry_backoff                  backoff=60 error=ConnectionError('Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download')\n",
      "2025-07-20T11:32:09.026248Z [error    ] ollama_connection_failed       url=http://10.0.0.4:8000\n",
      "2025-07-20T11:32:09.027038Z [error    ] Error processing document completion document_id=UUID('0685e25c-97db-7888-8000-d1ca912627bc') error=\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/client.py:51\u001b[39m, in \u001b[36minit_client\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[43mretry_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/client.py:46\u001b[39m, in \u001b[36mretry_backoff\u001b[39m\u001b[34m(timeout, func, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (k, document) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n\u001b[32m     22\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mdocument_loop\u001b[39m\u001b[33m\"\u001b[39m, document=document, index=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         result = \u001b[43mprocess_document_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_model_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m         completions_by_type[document.document_type].append(result)\n\u001b[32m     26\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mget_aggregations_for_document_types\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/completions.py:43\u001b[39m, in \u001b[36mprocess_document_completion\u001b[39m\u001b[34m(document, prompt, model_config)\u001b[39m\n\u001b[32m     40\u001b[39m messages = format_document_messages(prompt, document)\n\u001b[32m     42\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mdocument_completion_start\u001b[39m\u001b[33m\"\u001b[39m, company=company.ticker, year=filing.fiscal_year, document_type=document.document_type.value)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m response = \u001b[43mget_chat_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Extract necessary details from the response\u001b[39;00m\n\u001b[32m     46\u001b[39m created_at = response.created_at\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/client.py:67\u001b[39m, in \u001b[36mget_chat_response\u001b[39m\u001b[34m(model_config, messages, client)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_chat_response\u001b[39m(\n\u001b[32m     61\u001b[39m         model_config: ModelConfig,\n\u001b[32m     62\u001b[39m         messages: List[Dict],\n\u001b[32m     63\u001b[39m         client: Optional[Client] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     64\u001b[39m     ) -> ChatResponse:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         client = \u001b[43minit_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopenai_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     contents = [msg[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[32m     70\u001b[39m     input_tokens = \u001b[38;5;28msum\u001b[39m(count_tokens(model_config, content) \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/symbology/src/llm/client.py:54\u001b[39m, in \u001b[36minit_client\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m     53\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mollama_connection_failed\u001b[39m\u001b[33m\"\u001b[39m, url=url)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[32m     56\u001b[39m logger.debug(\u001b[33m'\u001b[39m\u001b[33mconnected_to_ollama\u001b[39m\u001b[33m'\u001b[39m, url=url)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "\u001b[31mTimeoutError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for (i, ticker) in enumerate(TICKERS):\n",
    "    # break\n",
    "    logger.info(\"get_completion_for_filing_documents\", company=company.name, ticker=company.ticker, index=f\"{i}/{len(TICKERS)}\")\n",
    "    LOOP_START = time.time()\n",
    "    company = get_company_by_ticker(ticker)\n",
    "    filings = get_filings_by_company(company.id)\n",
    "    # filings = [filing for filing in filings if filing.period_of_report.year > 2023]\n",
    "\n",
    "    # store output to make aggregate from\n",
    "    completions_by_type: defaultdict[DocumentType, List[Completion]] = defaultdict(list)\n",
    "\n",
    "    logger.debug(\"document_prompt\", content=f\"'{document_prompt.content.strip()[:90]}...'\")\n",
    "\n",
    "    for (j, filing) in enumerate(filings):\n",
    "        logger.info(\"filing_loop\", filing=filing, index=f\"{j}/{len(filings)}\")\n",
    "        documents = get_documents_by_filing(filing.id)\n",
    "        if len(documents) < 1:\n",
    "            logger.warning(\"no documents\")\n",
    "            continue\n",
    "\n",
    "        for (k, document) in enumerate(documents):\n",
    "            logger.info(\"document_loop\", document=document, index=f\"{k}/{len(filings)}\")\n",
    "            result = process_document_completion(document, document_prompt, document_model_config)\n",
    "            completions_by_type[document.document_type].append(result)\n",
    "\n",
    "    logger.info(\"get_aggregations_for_document_types\")\n",
    "    logger.debug(\"aggregate_prompt\", content=f\"'{aggregate_prompt.content.strip()[:90]}...'\")\n",
    "\n",
    "    logger.info(\"get_mda_aggregate\")\n",
    "    mda_aggregate = create_document_type_aggregate(DocumentType.MDA, completions_by_type[DocumentType.MDA], aggregate_prompt, aggregate_model_config)\n",
    "\n",
    "    logger.info(\"get_business_description_aggregate\")\n",
    "    description_aggregate = create_document_type_aggregate(DocumentType.DESCRIPTION, completions_by_type[DocumentType.DESCRIPTION], aggregate_prompt, aggregate_model_config)\n",
    "\n",
    "    # use a different prompt for risk factors\n",
    "    logger.info(\"get_risk_factors_aggregate\")\n",
    "    logger.info(\"risk_aggregate_prompt\", content=f\"'{risk_aggregate_prompt.content.strip()[:90]}...'\")\n",
    "    risk_aggregate = create_document_type_aggregate(DocumentType.RISK_FACTORS, completions_by_type[DocumentType.RISK_FACTORS], risk_aggregate_prompt, aggregate_model_config)\n",
    "\n",
    "\n",
    "    # Make the company 'front page summary' text\n",
    "    # remove <thinking>, format markdown, lint for misclanious wrapper text\n",
    "    logger.info(\"formatting_content\")\n",
    "    unformatted_markdown = remove_thinking_tags(description_aggregate.content)\n",
    "    with open('unformatted_markdown.md', 'w') as f:\n",
    "        f.write(unformatted_markdown)\n",
    "\n",
    "    markdown_response = get_generate_response(markdown_model_config, markdown_prompt.content, unformatted_markdown)\n",
    "    with open('formatted_markdown.md', 'w') as f:\n",
    "        f.write(markdown_response.response)\n",
    "\n",
    "    summary_response = get_generate_response(front_page_model_config, front_page_prompt.content, markdown_response.response)\n",
    "    linted_summary = get_generate_response(markdown_model_config, \"We are cleaning up for publication. Remove any introductory 'meta' lines from the provided document.\", summary_response.response)\n",
    "    company = update_company(company.id, {'summary': linted_summary.response})\n",
    "\n",
    "    LOOP_END = time.time()\n",
    "    logger.info(\"finished_company\", duration=f\"{LOOP_END - LOOP_START:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32388f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # regenerate an aggregate\n",
    "# from src.database.aggregates import Aggregate\n",
    "# from src.llm.prompts import format_aggregate_messages\n",
    "\n",
    "# # Get Aggregates where `aggregate.company.id == company.id` and `aggregate.document_type == document_type`\n",
    "# company = get_company_by_ticker(COMPANY)\n",
    "# document_type = DocumentType.DESCRIPTION\n",
    "\n",
    "# aggregates = (\n",
    "#     session.query(Aggregate)\n",
    "#     .filter(Aggregate.company_id == company.id)\n",
    "#     .filter(Aggregate.document_type == document_type)\n",
    "#     .all()\n",
    "# )\n",
    "\n",
    "# logger.info(\"retrieved_aggregates\", company=company.ticker, document_type=document_type.value, count=len(aggregates))\n",
    "# aggregate = max(aggregates, key=lambda agg: agg.created_at)\n",
    "# logger.info(\"most_recent_aggregate\", aggregate_id=aggregate.id, created_at=str(aggregate.created_at))\n",
    "# completions = aggregate.source_completions\n",
    "# logger.info(\"comlpetions\", length=len(completions))\n",
    "\n",
    "# # check the formatting of context\n",
    "# # formatted_context = format_aggregate_messages(aggregate_prompt, completions)\n",
    "# # print(formatted_context[1]['content'])\n",
    "\n",
    "# logger.info(\"aggregate_prompt\", content=f\"'{aggregate_prompt.content.strip()[:80]}...'\")\n",
    "# new_agg = create_document_type_aggregate(document_type, completions, aggregate_prompt, aggregate_model_config)\n",
    "# new_agg.content = remove_thinking_tags(new_agg.content)\n",
    "# with open('unformatted_markdown.md', 'w') as f:\n",
    "#     f.write(new_agg.content)\n",
    "\n",
    "# logger.info(\"formatting_markdown\")\n",
    "# new_agg = get_generate_response(markdown_model_config, markdown_prompt.content, new_agg.content)\n",
    "# new_agg = get_generate_response(markdown_model_config, \"We are cleaning up for publication. Remove any introductory 'meta' lines from the provided document.\", new_agg.response)\n",
    "# new_agg = get_generate_response(markdown_model_config, \"This document will be presented with a separate heading. Trim \", new_agg.response)\n",
    "# with open('formatted_markdown.md', 'w') as f:\n",
    "#     f.write(new_agg.response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAL', 'AAPL', 'ACHR', 'AES', 'AGNC', 'AMD', 'AMZN', 'APLD', 'AUR', 'AVGO', 'BA', 'BAC', 'CCL', 'CDE', 'CLF', 'CLSK', 'CSCO', 'CVE', 'F', 'GME', 'GOOGL', 'HBANP', 'HL', 'HOOD', 'HPE', 'INTC', 'IONQ', 'IPG', 'JOBY', 'KGC', 'KVUE', 'LCID', 'MARA', 'MP', 'MSFT', 'MU', 'NFLX', 'NU', 'NVDA', 'ORCL', 'PCG', 'PFE', 'PLTR', 'PR', 'QUBT', 'RIG', 'RIOT', 'RKLB', 'RKT', 'RXRX', 'S', 'SMCI', 'SMR', 'SNAP', 'SOFI', 'T', 'TSLA', 'UBER', 'UNH', 'VZ', 'WBD', 'WFC', 'WMT', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "from src.database.companies import get_all_company_tickers\n",
    "\n",
    "companies = get_all_company_tickers()\n",
    "print(companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = filings[0].documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71093f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_description"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.document_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4bae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMD 2020 10-K business_description"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630372bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD 2020 10-K business_description\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b337a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
